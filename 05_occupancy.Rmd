---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Modeling Occupancy {#occupancy}

## Introduction {#occupancy-intro}

In this chapter, we'll cover the basic steps for estimating **occupancy probability** using data from eBird. In Chapter \@ref(encounter), we used analytical approaches that accounted for variation in detectability by including covariates that are known to influence detection rates (e.g. effort). In contrast, occupancy models jointly model the ecological process of species occurrence *and* explicitly the observation process of species detection. The application of these models typically requires repeated visits to a single site during a relatively short time frame, over which the population can be considered closed. However, it is possible to retroactively create this repeat visit structure with eBird data to generate a subset of data suitable for estimating occupancy. Here, we discuss how to process eBird observations to meet criteria for the application of occupancy models. To illustrate our example, we apply single-season occupancy models to estimate occupancy and detection probabilities of Wood Thrush for the month of June in BCR 27. 

This chapter describes a method that is distinct from the previous chapter on modeling encounter rate in two important ways. First, the encounter rate model did not estimate absolute detectability and so only estimated average *encounter rate*. These occupancy models explicitly estimate detectability, which enables us to estimate *occupancy rate*. Second, the random forest model used in Chapter \@ref(encounter) is an example of a [machine learning](https://en.wikipedia.org/wiki/Machine_learning) approach, which is best suited for addressing specific questions and hypotheses, while the goal of machine learning is primarily to identify patterns and make predictions [@bzdokPointsSignificanceStatistics2018]. Additionally, machine learning approaches can accommodate complex interactions between covariates and non-linear effects, often needed to model habitat associations that can vary across large spatial and temporal scales. In contrast, occupancy models described here are more suitable for testing hypotheses, and exploring linear effects and simpler interactions. In this example, we specifically focus on the mechanics of filtering and formatting the data to produce data suitable for occupancy models, and less on how to choose which predictors to include for detection and occupancy probabilities. The predictors we do include are informed by the random forest model in Chapter \@ref(encounter). 

If you worked through the previous chapters, you should have all the data necessary for this chapter. You can also [download the data package](https://github.com/mstrimas/ebird-best-practices/raw/master/data/data.zip), and unzip it to your working directory. Note that the Checklist Calibration Index (CCI), which calibrates observers and checklists against others from similar times and places, is an optional covariate in these models. Including CCI typical leads to marked improvement in model performance; however, we are still working on the best way of publicly releasing these data, so they are not currently available. This section will be updated soon with download instructions once they become available. However, the code in this chapter will automatically run the models without this variable. 

```{r occupancy-data}
library(auk)
library(lubridate)
library(sf)
library(dggridR)
library(unmarked)
library(raster)
library(viridis)
library(MuMIn)
library(AICcmodavg)
library(fields)
library(tidyverse)
# resolve namespace conflicts
select <- dplyr::select
projection <- raster::projection

set.seed(1)

# setup output directory for saved results
if (!dir.exists("output")) {
  dir.create("output")
}

# ebird data
ebird <- read_csv("data/ebd_woothr_june_bcr27_zf.csv") %>% 
  mutate(year = year(observation_date),
         # occupancy modeling requires an integer response
         species_observed = as.integer(species_observed))

# modis land cover covariates
habitat <- read_csv("data/modis_pland_location-year.csv") %>% 
  mutate(year = as.integer(year))

# combine ebird and modis data
ebird_habitat <- inner_join(ebird, habitat, by = c("locality_id", "year"))

# optional checklist calibration index
cci_file <- "data/cci_june_bcr27.csv"
if (file.exists(cci_file)) {
  cci <- read_csv("data/cci_june_bcr27.csv")
  ebird_habitat <- inner_join(ebird_habitat, cci, by = "checklist_id") %>% 
    filter(!is.na(checklist_calibration_index))
}

# prediction surface
pred_surface <- read_csv("data/modis_pland_prediction-surface.csv")
r <- raster("data/prediction-surface.tif")

# load gis data for making maps
map_proj <- st_crs(102003)
ne_land <- read_sf("data/gis-data.gpkg", "ne_land") %>% 
  st_transform(crs = map_proj) %>% 
  st_geometry()
bcr <- read_sf("data/gis-data.gpkg", "bcr") %>% 
  st_transform(crs = map_proj) %>% 
  st_geometry()
ne_country_lines <- read_sf("data/gis-data.gpkg", "ne_country_lines") %>% 
  st_transform(crs = map_proj) %>% 
  st_geometry()
ne_state_lines <- read_sf("data/gis-data.gpkg", "ne_state_lines") %>% 
  st_transform(crs = map_proj) %>% 
  st_geometry()
```

## Data preparation {#occupancy-data}

First, we need to extract a subset of the eBird data suitable for occupancy modeling, then perform spatiotemporal subsampling to deal with bias in the data. Let's start by filtering our data to include only checklists with 5 or fewer observers, since there are very few checklists with more than 5 observers.

```{r occupancy-data-obs}
ebird_filtered <- filter(ebird_habitat, number_observers <= 5)
```

In some situations, you may want to further filter the data based on the results of an exploratory analysis similar to the one conducted in Section \@ref(ebird-explore). However, for the purpose of comparing results among different modeling approaches and best practices, we won't further filter the observations in eBird for our occupancy example. In addition, given the extra constraints for data suitable for occupancy modeling, it may be suitable to retain more checklists at this stage. 

### Extracting occupancy data {#occupancy-data-sites}

From the eBird data, we can generate detection histories for each location we define as a site. In this example, we define the month of June as the time period over which we assume that the population is closed for Wood Thrush, and a site is defined as a location (`locality_id` in the data) that is visited at least twice by the same observer within our defined period of closure (i.e. the month of June).

The `auk` function `filter_repeat_visits()` is designed to extract a subset of eBird data suitable for occupancy modeling. We first filter the data to only sites that have at least two visits (`min_obs = 2`), then define the maximum length of our detection history as 10 visits or checklists (`max_obs = 10`). When a specific site has been visited more than 10 times, the function will randomly select 10 checklists from the total number of visits. Since we only have data from June, using `annual_closure = TRUE` defines the temporal period of closure as the whole month of June within each year. Using this structure, the occurrence rate is allowed to change at sites between years. Finally, `site_vars` specifies the set of variables that defines a site. In this example, a site is defined jointly by location and observer IDs.

```{r occupancy-data-sites-repeat}
occ <- filter_repeat_visits(ebird_filtered, 
                            min_obs = 2, max_obs = 10,
                            annual_closure = TRUE,
                            date_var = "observation_date",
                            site_vars = c("locality_id", "observer_id"))
# entire data set
nrow(ebird_habitat)
# reduced data set
nrow(occ)
# how many individual sites there are
n_distinct(occ$site)
```

Three new variables are added to the dataset by the function `filter_repeat_visits()`: `site` is a unique site ID, `closure_id` identifies the primary period of closure (in this case the year), and `n_observations` is the number of visits to each site in this dataset. Our data are now formatted and ready to be analyzed using occupancy models. Note that we've made a tradeoff in sample size; selecting a set of data that are suitable for occupancy modeling means we dropped from `r scales::comma(nrow(ebird_filtered))` checklists to `r scales::comma(nrow(occ))` checklists over `r scales::comma(n_distinct(occ$site))` sites. 

We'll fit single-season occupancy models using the `unmarked` R package. For further details on the type of data format required for this package, consult the documentation for the `unmarked` function `formatWide()`. The `auk` function `format_unmarked_occu()` converts data from a vertical format in which each row is an observation (as in the EBD) to a horizontal detection history required by `unmarked`, where each row is a site. At this stage, we need to specify which variables will be site-level covariates and which will be observation-level covariates. Covariates for each site (`site_covs`) are specific to each primary sampling period of closure (month of June in this case), and will be associated with the ecological process of species occurrence. Covariates collected during each sampling occasion (`obs_covs`) are those that are specific to that sampling occasion (i.e. checklist) and will be associated with the observational process, or detection probability.

For this example, we'll use MODIS land cover variables as covariates for modeling the occupancy probability of Wood Thrush. Based on [predictor importance](#encounter-habitat-pi) measures from Chapter \@ref(encounter), we include deciduous broadleaf forest (`pland_04`) and mixed forest (`pland_05`) as habitat types for which we expect positive relationships with occupancy, and croplands (`pland_12`) and urban (`pland_13`), for which we expect negative relationships. 

For detection probability, we include the six effort variables, since they are related to the detection process. In addition, habitat is known to affect detectability, and many species are harder to detect in densely forested habitats—the same habitats preferred by Wood Thrush. With sufficient data, occupancy models allow us to tease apart the differing effects of habitat on detection and occupancy. With this in mind, we'll include deciduous broadleaf forest (`pland_04`) and mixed forest (`pland_05`) as additional detection covariates.

```{r occupancy-data-sites-wide}
# if cci is available, use it as an observation covariate
obs_covs <- c("time_observations_started", 
              "duration_minutes", 
              "effort_distance_km", 
              "number_observers", 
              "protocol_type",
              "pland_04", 
              "pland_05")
if ("checklist_calibration_index" %in% names(occ)) {
  obs_covs <- c(obs_covs, "checklist_calibration_index")
}

# format for unmarked
occ_wide <- format_unmarked_occu(occ, 
                                 site_id = "site", 
                                 response = "species_observed",
                                 site_covs = c("n_observations", 
                                               "latitude", "longitude", 
                                               # % deciduous forest
                                               "pland_04", 
                                               # % mixed forest
                                               "pland_05",
                                               # % cropland
                                               "pland_12",
                                               # % urban
                                               "pland_13"),
                                 obs_covs = obs_covs)
```

### Spatial subsampling {#encounter-data-sss}

As discussed in Section \@ref(encounter-sss), spatial subsampling of eBird observations reduces spatial bias. We'll use the same hexagonal subsampling approach as in Chapter \@ref(encounter); however, here we'll subsample at the level of sites rather than observations. This process will select only one site within each hexagon. It will reduce the influence of popular sites that are visited multiple times by different observers and therefore have several rows in the dataset for each geographic site. 

```{r encounter-data-sss, results = "hide"}
# generate hexagonal grid with ~ 5 km betweeen cells
dggs <- dgconstruct(spacing = 5)
# get hexagonal cell id for each site
occ_wide_cell <- occ_wide %>% 
  mutate(cell = dgGEO_to_SEQNUM(dggs, longitude, latitude)$seqnum)
# sample one checklist per grid cell
occ_ss <- occ_wide_cell %>% 
  group_by(cell) %>% 
  sample_n(size = 1) %>% 
  ungroup() %>% 
  select(-cell)
```

This resulted in a `r scales::percent(1 - nrow(occ_ss) / nrow(occ_wide))` decrease in the number of sites, but produced a more spatially balanced dataset without greater influence from more popular sites. 

### `unmarked` object {#encounter-data-unmarked}

Finally, we'll convert this dataframe of observations into an `unmarked` object, which is required for fitting occupancy models.

```{r encounter-data-unmarked}
occ_um <- formatWide(occ_ss, type = "unmarkedFrameOccu")
summary(occ_um)
```

## Occupancy modeling {#occupancy-model}

Now that we've created a data frame with detection histories and covariates, we can use `unmarked` to fit a single-season occupancy model. In this book, we won't delve into the mechanics of occupacy models; however, there is a rich literature on occupancy modeling and readers wishing to learn more about this field may want to consult the book on the topic by MacKenzie et al. [-@mackenzieOccupancyEstimationModeling2017]. Here we simply fit a single-season occupancy model to our data using the `occu()` function, specifying the detection and occupancy covariates, respectively, via a double right-hand sided formula of the form `~ detection covariates ~ occupancy covariates`. If checklist_calibration_index is not in the dataset, we use a formula without that variable. 

```{r occupancy-model-fit}
# use cci in model formula if available
if ("checklist_calibration_index" %in% names(occ_um@obsCovs)) {
  mod_formula <- ~ time_observations_started + 
                    duration_minutes + 
                    effort_distance_km + 
                    number_observers + 
                    protocol_type +
                    checklist_calibration_index +
                    pland_04 + pland_05 ~ 
                  pland_04 + pland_05 + pland_12 + pland_13
} else {
    mod_formula <- ~ time_observations_started + 
                    duration_minutes + 
                    effort_distance_km + 
                    number_observers + 
                    protocol_type +
                    pland_04 + pland_05 ~
                  pland_04 + pland_05 + pland_12 + pland_13
}

# fit model
occ_model <- occu(mod_formula, data = occ_um)
# look at the regression coefficients from the models
summary(occ_model)
```

### Assessment {#occupancy-model-assess}

Now we have fitted the model we want to assess whether it is a reasonable fit to the data. Although few goodness-of-fit tests exist for occupancy models, we'll demonstrate how to perform the MacKenzie and Bailey [-@mackenzieAssessingFitSiteoccupancy2004] goodness-of-fit test.  This approach calculates a Pearson's chi-square fit statistic from the observed and expected frequencies of detection histories for a given model. For this example, we'll use the `mb.gof.test()` test function in the `AICcmodavg` package, which can handle occupancy models produced by the `occu()` function in `unmarked`. Note that this process requires simulating a large number of bootstrap samples (1,000 here) and therefore takes a long time to run. You may want to skip this section or reduce `nsim` to a much smaller number (e.g. 10) in the interest of speed. However, when applying this in practice, you should run the test with the full number of simulations to get accurate results.

```{r occupancy-model-assess, eval = FALSE, echo = 1:2}
occ_gof <- mb.gof.test(occ_model, nsim = 1000, plot.hist = FALSE)
print(occ_gof)
saveRDS(occ_gof, "output/woothr_occupancy-model_gof.rds")
```

```{r occupancy-model-assess-actual, echo = FALSE}
# read in saved gof test results 
occ_gof <- readRDS("output/woothr_occupancy-model_gof.rds")
# print chisq table
gof <- occ_gof
gof$chisq.table <- NULL
print(gof)
```

For this example, the probability of getting the calculated chi-square statistic under a null sampling distribution is indicated by the p-value of `r gof$p.value`, indicating that there is no reason to consider a lack of fit (p > 0.1). In addition, we also get an estimate of the overdisperson parameter (c-hat) for the model by dividing the observed chi-square statistic by the mean of the statistics obtained from simulation. In this example, c-hat = `r round(gof$c.hat.est, 2)`, which is very close to c-hat = 1, indicating that the variance is not greater than the mean, and that there is no evidence for overdispersion. Together these suggest that there is not evidence of lack of fit of this model to these data . 

### Model selection {#occupancy-model-select}

Now we have verified the first model is a good fit, we next use a model selection approach to compare and rank our candidate model set. For this example, we use the `dredge()` function, which evaluates a set of candidate models generated by using different combinations of the terms in the global model. Ideally, we would explore all possible additive combinations of the global model; however, this would result in $2^{12}=4096$ models to evaluate, since the global model has 12 terms. It's not feasible to evaluate such a large candidate set in a reasonable amout of time, so we'll only explore a subset of possible models in this example. In particular, we'll lock in the effort variables, since we know from prior experience that these are almost always important. 

```{r occupancy-model-select-dredge}
# get list of all possible terms, then subset to those we want to keep
det_terms <- getAllTerms(occ_model) %>% 
  discard(str_detect, pattern = "pland_")

# dredge, fixing the effort covariates
occ_dredge <- dredge(occ_model, fixed = det_terms)

# model comparison
select(occ_dredge, df, logLik, AICc, delta, weight) %>% 
  top_n(10)
```

The corrected [Akaike Information Criterion (AICc)](https://en.wikipedia.org/wiki/Akaike_information_criterion#AICc) measures the performance of each model, relative to the other models in the candidate set, adjusting for the number of parameters. Lower values indicate models with a better fit to the data, penalizing for the number of parameters. Delta is the difference between the AICc values for the given model and that for the top model (i.e. the one with the lowest AICc). Finally, the AIC weight is a transformation of delta that can be interpreted as the probability that the given model is the most likely one of the candidate models to have generated the data.

A quick look at the dredge object reveals that for the Wood Thrush example there is not a clear single model, or even a small set of models, that are most likely to have generated our data. This is evident from the low AIC weight for the top model and the large number of models with moderate AIC weights. Given this, and the fact that all of our effects are linear and use the same family and link function, we'll average across all models, weighted by AICc, to produce a model-averaged prediction. However, there may be scenarios in which there is a clear set of high performing models, in which case you can use the `get.models()` function to extract just these models prior to averaging. For the sake of efficiency, we'll only average the top models, which we'll define as those cumulatively comprising 95% of the weights. This won't impact the results since the models with lower support have such small weights and therefore contribute little to the predictions.

```{r occupancy-model-select-average}
# select models with the most suport for model averaging
occ_dredge_95 <- get.models(occ_dredge, subset = cumsum(weight) <= 0.95)

# average models based on model weights 
occ_avg <- model.avg(occ_dredge_95, fit = TRUE)

# model coefficients
t(occ_avg$coefficients)
```

## Prediction {#occupancy-predict}

In this section, we'll estimate the distribution of Wood Thrush in BCR 27. Similar to Section \@ref(habitat-prediction), we'll generate a prediction surface using the PLAND land cover covariates summarized on a regular grid of points across BCR 27. For this, we'll use the `predict()` function to estimate occupancy probabilities, standard errors, and confidence intervals. When we use `predict()` on the output of `get.models()` it will make predictions for each of the selected models, then average the predictions using the AIC weights to produce the final prediction.

Recall that when we [predicted encouter rate](#encounter-predict), we had to include effort variables in our prediction surface. We don't need to do that here because the occupancy submodel doesn't depend on the effort covariates, these only occur in the detection submodel.

```{r occupancy-predict-predict, eval = FALSE, results = "hold", echo = 1:9}

if (.Platform$OS == "unix") {
  Sys.time()
} else {system.time()}

occ_pred <- predict(occ_avg, 
                    newdata = as.data.frame(pred_surface), 
                    type = "state")

# add to prediction surface
pred_occ <- bind_cols(pred_surface, 
                      occ_prob = occ_pred$fit, 
                      occ_se = occ_pred$se.fit) %>% 
  select(latitude, longitude, occ_prob, occ_se)
saveRDS(pred_occ, "output/woothr_occupancy-model_predictions.rds")
# above code takes 35 minutes to run
```

```{r occupancy-predict-predict-load, echo = FALSE}
pred_occ <- readRDS("output/woothr_occupancy-model_predictions.rds")
```

Next, we'll convert this data frame to spatial feaatures using `sf`, then rasterize the points using the prediction surface raster template.

```{r occupancy-predict-rasterize}
r_pred <- pred_occ %>% 
  # convert to spatial features
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>% 
  st_transform(crs = projection(r)) %>% 
  # rasterize
  rasterize(r)
r_pred <- r_pred[[c("occ_prob", "occ_se")]]

# save the raster
tif_dir <- "output"
if (!dir.exists(tif_dir)) {
  dir.create(tif_dir)
}
writeRaster(r_pred[["occ_prob"]], 
            filename = file.path(tif_dir, "occupancy-model_prob_woothr.tif"),
            overwrite = TRUE)
writeRaster(r_pred[["occ_se"]], 
            filename = file.path(tif_dir, "occupancy-model_se_woothr.tif"), 
            overwrite = TRUE)
```

Finally, we can map these predictions!

```{r occupancy-predict-map, fig.asp = 1.236}
# project predictions
r_pred_proj <- projectRaster(r_pred, crs = map_proj$proj4string, method = "ngb")

par(mfrow = c(2, 1))
for (nm in names(r_pred)) {
  r_plot <- r_pred_proj[[nm]]
  
  par(mar = c(3.5, 0.25, 0.25, 0.25))
  # set up plot area
  plot(bcr, col = NA, border = NA)
  plot(ne_land, col = "#dddddd", border = "#888888", lwd = 0.5, add = TRUE)
  
  # modified plasma palette
  plasma_rev <- rev(plasma(25, end = 0.9))
  gray_int <- colorRampPalette(c("#dddddd", plasma_rev[1]))
  pal <- c(gray_int(4)[2], plasma_rev)
  
  # occupancy
  mx <- ceiling(1000 * cellStats(r_plot, max)) / 1000
  brks <- seq(0, mx, length.out = length(pal) + 1)
  plot(r_plot, 
       col = pal, breaks = brks, 
       maxpixels = ncell(r_plot),
       legend = FALSE, add = TRUE)
  
  # borders
  plot(bcr, border = "#000000", col = NA, lwd = 1, add = TRUE)
  plot(ne_state_lines, col = "#ffffff", lwd = 0.75, add = TRUE)
  plot(ne_country_lines, col = "#ffffff", lwd = 1.5, add = TRUE)
  box()
  
  # legend
  par(new = TRUE, mar = c(0, 0, 0, 0))
  if (nm == "occ_prob") {
    title <- "Wood Thrush Occupancy Probability"
    lbl_brks <- seq(0, mx, by = 0.1)
  } else {
    lbl_brks <- seq(0, mx, by = 0.02)
    title <- "Wood Thrush Occupancy Uncertainty (SE)"
  }
  
  image.plot(zlim = range(brks), legend.only = TRUE, col = pal,
             smallplot = c(0.25, 0.75, 0.06, 0.09),
             horizontal = TRUE,
             axis.args = list(at = lbl_brks, labels = lbl_brks,
                              fg = "black", col.axis = "black",
                              cex.axis = 0.75, lwd.ticks = 0.5,
                              padj = -1.5),
             legend.args = list(text = title,
                                side = 3, col = "black",
                                cex = 1, line = 0))
}
```
